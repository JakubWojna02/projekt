{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3da7d205-dfc2-4293-952a-4ee697d9bfa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /local_disk0/.ephemeral_nfs/envs/pythonEnv-647ba525-abc5-450e-9d88-79d0f28e7767/lib/python3.12/site-packages (3.1.1)\nRequirement already satisfied: scikit-learn in /databricks/python3/lib/python3.12/site-packages (1.6.1)\nRequirement already satisfied: hyperopt in /local_disk0/.ephemeral_nfs/envs/pythonEnv-647ba525-abc5-450e-9d88-79d0f28e7767/lib/python3.12/site-packages (0.2.7)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.12/site-packages (1.4.2)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.12/site-packages (2.2.3)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.12/site-packages (from xgboost) (2.1.3)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.12/site-packages (from xgboost) (1.15.1)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: six in /usr/lib/python3/dist-packages (from hyperopt) (1.16.0)\nRequirement already satisfied: networkx>=2.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-647ba525-abc5-450e-9d88-79d0f28e7767/lib/python3.12/site-packages (from hyperopt) (3.5)\nRequirement already satisfied: future in /local_disk0/.ephemeral_nfs/envs/pythonEnv-647ba525-abc5-450e-9d88-79d0f28e7767/lib/python3.12/site-packages (from hyperopt) (1.0.0)\nRequirement already satisfied: tqdm in /local_disk0/.ephemeral_nfs/envs/pythonEnv-647ba525-abc5-450e-9d88-79d0f28e7767/lib/python3.12/site-packages (from hyperopt) (4.67.1)\nRequirement already satisfied: cloudpickle in /databricks/python3/lib/python3.12/site-packages (from hyperopt) (3.0.0)\nRequirement already satisfied: py4j in /databricks/python3/lib/python3.12/site-packages (from hyperopt) (0.10.9.9)\nRequirement already satisfied: python-dateutil>=2.8.2 in /databricks/python3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /databricks/python3/lib/python3.12/site-packages (from pandas) (2024.1)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install xgboost scikit-learn hyperopt joblib pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3c98325-b766-4123-aa25-57b1ae3b836f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /local_disk0/.ephemeral_nfs/envs/pythonEnv-52889a4a-959a-48e5-a735-4c28b071a7d0/lib/python3.12/site-packages (3.1.1)\nRequirement already satisfied: scikit-learn in /databricks/python3/lib/python3.12/site-packages (1.6.1)\nRequirement already satisfied: hyperopt in /local_disk0/.ephemeral_nfs/envs/pythonEnv-52889a4a-959a-48e5-a735-4c28b071a7d0/lib/python3.12/site-packages (0.2.7)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.12/site-packages (1.4.2)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.12/site-packages (2.2.3)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.12/site-packages (from xgboost) (2.1.3)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.12/site-packages (from xgboost) (1.15.1)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: six in /usr/lib/python3/dist-packages (from hyperopt) (1.16.0)\nRequirement already satisfied: networkx>=2.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-52889a4a-959a-48e5-a735-4c28b071a7d0/lib/python3.12/site-packages (from hyperopt) (3.5)\nRequirement already satisfied: future in /local_disk0/.ephemeral_nfs/envs/pythonEnv-52889a4a-959a-48e5-a735-4c28b071a7d0/lib/python3.12/site-packages (from hyperopt) (1.0.0)\nRequirement already satisfied: tqdm in /local_disk0/.ephemeral_nfs/envs/pythonEnv-52889a4a-959a-48e5-a735-4c28b071a7d0/lib/python3.12/site-packages (from hyperopt) (4.67.1)\nRequirement already satisfied: cloudpickle in /databricks/python3/lib/python3.12/site-packages (from hyperopt) (3.0.0)\nRequirement already satisfied: py4j in /databricks/python3/lib/python3.12/site-packages (from hyperopt) (0.10.9.9)\nRequirement already satisfied: python-dateutil>=2.8.2 in /databricks/python3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /databricks/python3/lib/python3.12/site-packages (from pandas) (2024.1)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n1. Ładowanie danych...\nDane gotowe: 6460 wierszy.\n✅ Skaler zapisany do: /tmp/eth_scaler.joblib\n\n--- Model 1: XGBoost ---\n\r  0%|          | 0/20 [00:00<?, ?trial/s, best loss=?]\r 10%|█         | 2/20 [00:00<00:02,  7.97trial/s, best loss: 0.0014281312772275055]\r 25%|██▌       | 5/20 [00:00<00:01, 12.76trial/s, best loss: 0.0014281312772275055]\r 35%|███▌      | 7/20 [00:00<00:01,  7.32trial/s, best loss: 0.0014281312772275055]\r 50%|█████     | 10/20 [00:01<00:00, 10.47trial/s, best loss: 0.0014281312772275055]\r 60%|██████    | 12/20 [00:01<00:00, 12.14trial/s, best loss: 0.0014281312772275055]\r 70%|███████   | 14/20 [00:01<00:00, 12.95trial/s, best loss: 0.0014281312772275055]\r 80%|████████  | 16/20 [00:01<00:00, 14.20trial/s, best loss: 0.0014281312772275055]\r 90%|█████████ | 18/20 [00:01<00:00, 11.99trial/s, best loss: 0.0014281312772275055]\r100%|██████████| 20/20 [00:02<00:00,  8.02trial/s, best loss: 0.0014281312772275055]\r100%|██████████| 20/20 [00:02<00:00,  9.87trial/s, best loss: 0.0014281312772275055]\n✅ XGBoost zapisany.\n--- Model 2: Bayes ---\n--- Model 3: Linear ---\n--- Model 4: k-NN ---\n\nGOTOWE. Modele i Skaler zaktualizowane.\n"
     ]
    }
   ],
   "source": [
    "# Trening ML + SCALER\n",
    "\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import BayesianRidge, LinearRegression \n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from pyspark.sql import functions as F\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "INPUT_TABLE = \"eth_raw_stream_max_15_sec\"\n",
    "# Ścieżki\n",
    "PATH_XGB = '/tmp/eth_model_xgboost.joblib'\n",
    "PATH_BAYES = '/tmp/eth_model_bayes.joblib'\n",
    "PATH_LINEAR = '/tmp/eth_model_linear.joblib'\n",
    "PATH_KNN = '/tmp/eth_model_knn.joblib'\n",
    "PATH_SCALER = '/tmp/eth_scaler.joblib'\n",
    "\n",
    "N_TRIALS = 20 \n",
    "\n",
    "def calculate_features_per_block(group):\n",
    "    group = group.sort_values(\"timestamp\")\n",
    "    group['log_returns'] = np.log(group['close'] / group['close'].shift(1))\n",
    "    group['returns_sma'] = group['log_returns'].rolling(window=12).mean()\n",
    "    group['returns_std'] = group['log_returns'].rolling(window=12).std()\n",
    "    \n",
    "    future_close = group['close'].shift(-12)\n",
    "    group['target_log_return'] = np.log(future_close / group['close'])\n",
    "    return group\n",
    "\n",
    "print(\"1. Ładowanie danych...\")\n",
    "try:\n",
    "    df_spark = spark.read.table(INPUT_TABLE)\n",
    "    pandas_df = df_spark.orderBy(\"timestamp\").toPandas()\n",
    "except Exception as e:\n",
    "    print(f\"Błąd: {e}\")\n",
    "    raise e\n",
    "\n",
    "pandas_df = calculate_features_per_block(pandas_df)\n",
    "pandas_df = pandas_df.dropna()\n",
    "\n",
    "print(f\"Dane gotowe: {len(pandas_df)} wierszy.\")\n",
    "\n",
    "if len(pandas_df) < 150:\n",
    "    print(\"ZA MAŁO DANYCH!\")\n",
    "else:\n",
    "    # 2. Skalowanie Danych (Kluczowe dla k-NN i Regresji)\n",
    "    features_cols = ['returns_sma', 'returns_std', 'volume']\n",
    "    \n",
    "    # Inicjalizujemy Skaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Dopasowujemy go do WSZYSTKICH danych (żeby znał średnią i odchylenie)\n",
    "    X_raw = pandas_df[features_cols].values\n",
    "    X_scaled = scaler.fit_transform(X_raw)\n",
    "    \n",
    "    Y = pandas_df['target_log_return'].values\n",
    "    \n",
    "    # Zapisujemy skaler, żeby Konsument (Notatnik C) mógł go użyć\n",
    "    joblib.dump(scaler, PATH_SCALER)\n",
    "    print(f\"Skaler zapisany do: {PATH_SCALER}\")\n",
    "\n",
    "    # Podział na Train/Test (używamy już przeskalowanych X)\n",
    "    total_len = len(X_scaled)\n",
    "    train_end = int(total_len * 0.85) # Więcej na trening\n",
    "    \n",
    "    X_train, Y_train = X_scaled[:train_end], Y[:train_end]\n",
    "    X_test, Y_test = X_scaled[train_end:], Y[train_end:]\n",
    "    \n",
    "    # --- MODEL 1: XGBoost ---\n",
    "    print(\"\\n--- Model 1: XGBoost ---\")\n",
    "    space = {\n",
    "        'max_depth': hp.choice('max_depth', range(3, 10)),\n",
    "        'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n",
    "        'n_estimators': hp.choice('n_estimators', range(50, 200, 10)),\n",
    "        'gamma': hp.uniform('gamma', 0, 0.2),\n",
    "    }\n",
    "    def objective(params):\n",
    "        model = XGBRegressor(**params, random_state=42, n_jobs=-1)\n",
    "        model.fit(X_train, Y_train) \n",
    "        rmse = np.sqrt(mean_squared_error(Y_test, model.predict(X_test)))\n",
    "        return {'loss': rmse, 'status': STATUS_OK}\n",
    "\n",
    "    best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=N_TRIALS, trials=Trials())\n",
    "    final_xgb = XGBRegressor(**best, random_state=42, n_jobs=-1)\n",
    "    final_xgb.fit(X_train, Y_train)\n",
    "    joblib.dump(final_xgb, PATH_XGB)\n",
    "    print(f\"XGBoost zapisany.\")\n",
    "    \n",
    "    # --- MODEL 2: Bayes ---\n",
    "    print(\"--- Model 2: Bayes ---\")\n",
    "    final_bayes = BayesianRidge() \n",
    "    final_bayes.fit(X_train, Y_train)\n",
    "    joblib.dump(final_bayes, PATH_BAYES)\n",
    "    \n",
    "    # --- MODEL 3: Linear ---\n",
    "    print(\"--- Model 3: Linear ---\")\n",
    "    final_linear = LinearRegression()\n",
    "    final_linear.fit(X_train, Y_train)\n",
    "    joblib.dump(final_linear, PATH_LINEAR)\n",
    "    \n",
    "    # --- MODEL 4: k-NN ---\n",
    "    print(\"--- Model 4: k-NN ---\")\n",
    "    final_knn = KNeighborsRegressor(n_neighbors=10, weights='distance')\n",
    "    final_knn.fit(X_train, Y_train)\n",
    "    joblib.dump(final_knn, PATH_KNN)\n",
    "\n",
    "    print(\"\\nGOTOWE. Modele i Skaler zaktualizowane.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6df5847e-18a5-400c-ae64-46e1f850eadc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Ładowanie Modeli i Skalera...\nBŁĄD: Brak skalera! Uruchom Notatnik B.\nBrak modelu: XGBoost\nBrak modelu: Bayes\nBrak modelu: Linear\nBrak modelu: kNN\n\uD83D\uDE80 Start Monitoringu.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:139)\n",
       "\tat scala.Option.getOrElse(Option.scala:201)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:139)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:136)\n",
       "\tat scala.collection.immutable.Range.foreach(Range.scala:192)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:136)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:721)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:441)\n",
       "\tat scala.Option.getOrElse(Option.scala:201)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:441)\n",
       "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:486)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:768)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:512)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:632)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$5(UsageLogging.scala:659)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:80)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:172)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:153)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:80)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:627)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:521)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:80)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:513)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:477)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:80)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:740)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur.handleDriverRequest$1(Chauffeur.scala:943)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur.$anonfun$handleDriverRequests$2(Chauffeur.scala:970)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:632)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$5(UsageLogging.scala:659)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur.withAttributionContext(Chauffeur.scala:167)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:172)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:153)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur.withAttributionTags(Chauffeur.scala:167)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:627)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:521)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur.recordOperationWithResultTags(Chauffeur.scala:167)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:969)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur.handleDriverRequests(Chauffeur.scala:1020)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$com$databricks$spark$chauffeur$Chauffeur$$nestedInanon$$receiveInternal$1.applyOrElse(Chauffeur.scala:828)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$com$databricks$spark$chauffeur$Chauffeur$$nestedInanon$$receiveInternal$1.applyOrElse(Chauffeur.scala:730)\n",
       "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:726)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:721)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:178)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:204)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:204)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:175)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:165)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:512)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:632)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$5(UsageLogging.scala:659)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:172)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:153)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:627)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:521)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:513)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:477)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.executeWithLogging$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:165)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:997)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:917)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:557)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:522)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$12(ActivityContextFactory.scala:1132)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:68)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$2(ActivityContextFactory.scala:1132)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:1094)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:1075)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$41(ActivityContextFactory.scala:437)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:68)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:437)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:522)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:417)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:111)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:46)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:111)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:46)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:93)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:139)",
        "\tat scala.Option.getOrElse(Option.scala:201)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:139)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:136)",
        "\tat scala.collection.immutable.Range.foreach(Range.scala:192)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:136)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:721)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:441)",
        "\tat scala.Option.getOrElse(Option.scala:201)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:441)",
        "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:486)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:768)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:512)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:632)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$5(UsageLogging.scala:659)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:80)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:172)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:153)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:80)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:627)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:521)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:80)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:513)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:477)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:80)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:740)",
        "\tat com.databricks.spark.chauffeur.Chauffeur.handleDriverRequest$1(Chauffeur.scala:943)",
        "\tat com.databricks.spark.chauffeur.Chauffeur.$anonfun$handleDriverRequests$2(Chauffeur.scala:970)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:632)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$5(UsageLogging.scala:659)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)",
        "\tat com.databricks.spark.chauffeur.Chauffeur.withAttributionContext(Chauffeur.scala:167)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:172)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:153)",
        "\tat com.databricks.spark.chauffeur.Chauffeur.withAttributionTags(Chauffeur.scala:167)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:627)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:521)",
        "\tat com.databricks.spark.chauffeur.Chauffeur.recordOperationWithResultTags(Chauffeur.scala:167)",
        "\tat com.databricks.spark.chauffeur.Chauffeur.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:969)",
        "\tat com.databricks.spark.chauffeur.Chauffeur.handleDriverRequests(Chauffeur.scala:1020)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$com$databricks$spark$chauffeur$Chauffeur$$nestedInanon$$receiveInternal$1.applyOrElse(Chauffeur.scala:828)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$com$databricks$spark$chauffeur$Chauffeur$$nestedInanon$$receiveInternal$1.applyOrElse(Chauffeur.scala:730)",
        "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:726)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:721)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:178)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:204)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:204)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:175)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:165)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:512)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:632)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$5(UsageLogging.scala:659)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:172)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:153)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:627)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:521)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:513)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:477)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.executeWithLogging$1(ServerBackend.scala:147)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:165)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:997)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:917)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:557)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:522)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$12(ActivityContextFactory.scala:1132)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:68)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$2(ActivityContextFactory.scala:1132)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:1094)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:1075)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$41(ActivityContextFactory.scala:437)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:68)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:437)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:522)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:417)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:111)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:46)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:111)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:46)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:93)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.base/java.lang.Thread.run(Thread.java:840)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NOTATNIK C: ULTIMATE COMPARATOR (Ze Skalowaniem)\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "# --- Konfiguracja ---\n",
    "INPUT_TABLE = \"eth_raw_stream_max_15_sec\" \n",
    "REFRESH_RATE = 5 \n",
    "# Ścieżki\n",
    "PATH_XGB = '/tmp/eth_model_xgboost.joblib'\n",
    "PATH_BAYES = '/tmp/eth_model_bayes.joblib'\n",
    "PATH_LINEAR = '/tmp/eth_model_linear.joblib'\n",
    "PATH_KNN = '/tmp/eth_model_knn.joblib'\n",
    "PATH_SCALER = '/tmp/eth_scaler.joblib' # <--- NOWOŚĆ\n",
    "\n",
    "# --- Klasa Trackera (Bez zmian) ---\n",
    "class ErrorTracker:\n",
    "    def __init__(self):\n",
    "        self.actuals = []\n",
    "        self.preds = []\n",
    "        self.naive_errors = []\n",
    "        self.last_actual = None\n",
    "    def update(self, actual, predicted):\n",
    "        if self.last_actual is not None:\n",
    "            naive_error = abs(actual - self.last_actual)\n",
    "            self.naive_errors.append(naive_error)\n",
    "        self.actuals.append(actual)\n",
    "        self.preds.append(predicted)\n",
    "        self.last_actual = actual\n",
    "        if len(self.actuals) > 100:\n",
    "            self.actuals.pop(0)\n",
    "            self.preds.pop(0)\n",
    "            self.naive_errors.pop(0)\n",
    "    def get_rmse(self):\n",
    "        if len(self.actuals) < 2: return 0.0\n",
    "        return np.sqrt(mean_squared_error(self.actuals, self.preds))\n",
    "    def get_mae(self):\n",
    "        if len(self.actuals) < 2: return 0.0\n",
    "        return mean_absolute_error(self.actuals, self.preds)\n",
    "    def get_mape(self):\n",
    "        if len(self.actuals) < 2: return 0.0\n",
    "        return mean_absolute_percentage_error(self.actuals, self.preds) * 100\n",
    "    def get_mase(self):\n",
    "        if len(self.actuals) < 2 or sum(self.naive_errors) == 0: return 0.0\n",
    "        return self.get_mae() / np.mean(self.naive_errors)\n",
    "\n",
    "# --- 1. ŁADOWANIE ZASOBÓW ---\n",
    "trackers = {name: ErrorTracker() for name in [\"XGBoost\", \"Bayes\", \"Linear\", \"kNN\", \"Stoch\"]}\n",
    "models = {}\n",
    "path_map = {\"XGBoost\": PATH_XGB, \"Bayes\": PATH_BAYES, \"Linear\": PATH_LINEAR, \"kNN\": PATH_KNN}\n",
    "\n",
    "print(\"1. Ładowanie Modeli i Skalera...\")\n",
    "try:\n",
    "    scaler = joblib.load(PATH_SCALER)\n",
    "    print(\"Skaler danych załadowany.\")\n",
    "except:\n",
    "    print(\"BŁĄD: Brak skalera! Uruchom Notatnik B.\")\n",
    "    scaler = None\n",
    "\n",
    "for name, path in path_map.items():\n",
    "    try:\n",
    "        models[name] = joblib.load(path)\n",
    "    except:\n",
    "        print(f\"Brak modelu: {name}\")\n",
    "\n",
    "# 2. Główna Funkcja Analizy\n",
    "def run_comparison_analysis(last_predictions_dict):\n",
    "    try:\n",
    "        df = spark.read.table(INPUT_TABLE)\n",
    "        if df.isEmpty() or df.count() < 20: return None\n",
    "        \n",
    "        pdf = df.orderBy(F.col(\"timestamp\").desc()).limit(100).toPandas()\n",
    "        pdf = pdf.sort_values(\"timestamp\")\n",
    "        \n",
    "        # --- Inżynieria ---\n",
    "        pdf['log_returns'] = np.log(pdf['close'] / pdf['close'].shift(1))\n",
    "        pdf['returns_sma'] = pdf['log_returns'].rolling(window=12).mean()\n",
    "        pdf['returns_std'] = pdf['log_returns'].rolling(window=12).std()\n",
    "        \n",
    "        current_price = pdf['close'].iloc[-1]\n",
    "        current_time = pdf['timestamp'].iloc[-1]\n",
    "        sigma = pdf['log_returns'].std() if not np.isnan(pdf['log_returns'].std()) else 0.001\n",
    "        \n",
    "        # --- PRZYGOTOWANIE WEJŚCIA ZE SKALOWANIEM ---\n",
    "        # Surowe cechy\n",
    "        X_raw = pdf[['returns_sma', 'returns_std', 'volume']].fillna(0).iloc[[-1]].values \n",
    "        \n",
    "        if scaler:\n",
    "            # Skalujemy tak samo jak przy treningu!\n",
    "            X_input = scaler.transform(X_raw)\n",
    "        else:\n",
    "            X_input = X_raw # Fallback (wyniki będą gorsze)\n",
    "        \n",
    "        preds = {}\n",
    "        \n",
    "        # --- PREDYKCJE ML ---\n",
    "        for name in ['XGBoost', 'Bayes', 'Linear', 'kNN']:\n",
    "            if name in models:\n",
    "                log_ret_pred = models[name].predict(X_input)[0]\n",
    "                preds[name] = current_price * np.exp(log_ret_pred)\n",
    "            else:\n",
    "                preds[name] = current_price\n",
    "\n",
    "        # --- MODEL STOCHASTYCZNY ---\n",
    "        drift_log = models['XGBoost'].predict(X_input)[0] if 'XGBoost' in models else 0.0\n",
    "        \n",
    "        brownian = np.random.normal(0, sigma, 1000)\n",
    "        jumps = np.random.choice([0, 1], size=1000, p=[0.95, 0.05])\n",
    "        jump_mag = np.random.normal(0, 3*sigma, 1000) * jumps\n",
    "        \n",
    "        sim_log_ret = drift_log + brownian + jump_mag\n",
    "        preds['Stoch'] = current_price * np.mean(np.exp(sim_log_ret))\n",
    "\n",
    "        # --- UPDATE ---\n",
    "        if last_predictions_dict:\n",
    "            for name, old_pred in last_predictions_dict.items():\n",
    "                trackers[name].update(current_price, old_pred)\n",
    "            \n",
    "        return {\"ts\": current_time, \"price\": current_price, \"preds\": preds}\n",
    "\n",
    "    except Exception as e:\n",
    "        # print(f\"Err: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Pętla ---\n",
    "print(f\"Start Monitoringu.\")\n",
    "last_preds_dict = None\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        res = run_comparison_analysis(last_preds_dict)\n",
    "        \n",
    "        if res:\n",
    "            rmses = {name: t.get_rmse() for name, t in trackers.items()}\n",
    "            winner = min(rmses, key=rmses.get)\n",
    "            \n",
    "            ts_str = res['ts'].strftime('%H:%M:%S')\n",
    "            print(f\"\\n[{ts_str}] CENA: {res['price']:.2f} USD\")\n",
    "            print(\"-\" * 90)\n",
    "            print(f\"{'MODEL':<10} | {'PROGNOZA':<10} | {'RMSE':<10} | {'MAE':<10} | {'MAPE':<10}\")\n",
    "            print(\"-\" * 90)\n",
    "            \n",
    "            for name in [\"XGBoost\", \"Bayes\", \"Linear\", \"kNN\", \"Stoch\"]:\n",
    "                marker = \"\uD83D\uDC51\" if name == winner else \"  \"\n",
    "                rmse = trackers[name].get_rmse()\n",
    "                mae = trackers[name].get_mae()\n",
    "                mape = trackers[name].get_mape()\n",
    "                \n",
    "                print(f\"{marker} {name:<8} | {res['preds'].get(name, 0.0):<10.2f} | {rmse:<10.4f} | {mae:<10.4f} | {mape:<10.4f}\")\n",
    "                \n",
    "            print(\"-\" * 90)\n",
    "            print(f\"\uD83C\uDFC6 Lider: {winner}\")\n",
    "            \n",
    "            last_preds_dict = res['preds']\n",
    "        else:\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "\n",
    "        time.sleep(REFRESH_RATE)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Zatrzymano.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6581931291474487,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Module_B",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}